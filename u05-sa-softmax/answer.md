1. Probabilities are positive and sum to one. Softmax uses an exponential to make the return values positive because the next stage in cross entropy loss calculations is negative log likelihood, and the log function is undefined for negative numbers. The outputs of a linear layer can return a wide range of values. Softmax normalizes the values, so they sum to one, otherwise loss calculations wouldn't be comparable to each other and lead to situations where the loss for an incorrect prediction is lower than for a correct prediction. 

2. The exponential function in softmax has a slope that increases rapidly for positive inputs, so higher confidence in incorrect predictions lead to larger loss than a linear function, and higher confidence in correct predictions lead to lower loss than a linear function. This helps the neural net converge faster to classifying outputs correctly.